<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>PhysMotion: Physics-Grounded Dynamics From a Single Image</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bootstrap.min.css">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/app.css">
  <link rel="icon" href="./static/images/icon.jpg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/video_comparison.js"></script>
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">PhysMotion: Physics-Grounded Dynamics From a Single Image</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://xpandora.github.io/">Xiyang Tan</a><sup>1,*</sup>,</span>
              <span class="author-block">
                <a href="https://yingjiang96.github.io/">Ying Jiang</a><sup>1,*</sup>,</span>
              <span class="author-block">
                <a href="https://xuan-li.github.io/">Xuan Li</a><sup>1,*</sup>,</span>
              <span class="author-block">
                <a href="https://www.math.ucla.edu/~zeshunzong/">Zeshun Zong</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://xpandora.github.io/">Tianyi Xie</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://yangzzzy.github.io/">Yin Yang</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.math.ucla.edu/~cffjiang/">Chenfanfu Jiang</a><sup>1</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>University of California, Los Angeles,</span>
              <span class="author-block"><sup>2</sup>University of Utah</span>
            </div>
            
            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>*</sup>Equal Contributions</span>
            </div>



            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <!-- <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2311.12198" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <!-- <span class="link-block">
                  <a
                    href="#"
                    class="external-link button is-normal is-rounded is-dark"
                    onclick="return false;"
                    style="cursor: not-allowed;"
                  >
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (coming soon)</span>
                  </a>
                </span> -->
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/teaser.mp4" type="video/mp4">
        </video> -->
        <img src="./static/images/teaser-1.png" class="interpolation-image" />
        <h2 class="subtitle has-text-centered">
          Leveraging intermediate 3D representation, <span class="dnerf">PhysMotion</span> is a novel framework that generates physics-grounded high-quality dynamics from only a single input image.
        </h2>
      </div>
    </div>
  </section>

  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-vase">
            <video poster="" id="vase" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/ours_vase.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-toy">
            <video poster="" id="toy" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/ours_toy.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-patrickstar">
            <video poster="" id="patrickstar" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/ours_diamond.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-patrickstar">
            <video poster="" id="patrickstar" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/ours_basketball.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-patrickstar">
            <video poster="" id="patrickstar" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/ours_patrickstar.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-chair">
            <video poster="" id="chair" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/ours_chair.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-foxhead">
            <video poster="" id="foxhead" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/ours_foxhead.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>
  
  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              We introduce <strong>PhysMotion</strong>, a novel framework that leverages principled physics-based simulations to guide intermediate 
              3D representations generated from a single image and input conditions (e.g. applied force and torque), producing high-quality, physically plausible 
              video generation. By utilizing continuum mechanics-based simulations as a prior knowledge, our approach addresses the limitations of traditional 
              data-driven generative models and result in more consistent physically plausible motions. Our framework begins by reconstructing a feed-forward 3D 
              Gaussian from a single image through geometry optimization. This representation is then time-stepped using a differentiable Material Point Method (MPM) 
              with continuum mechanics-based elastoplasticity models, which provides a strong foundation for realistic dynamics, albeit at a coarse level of detail. 
              To enhance the geometry, appearance, and ensure spatiotemporal consistency, we refine the initial simulation using a text-to-image (T2I) diffusion model 
              with cross-frame attention, resulting in a physically plausible video that retains intricate details comparable to the input image. We conduct comprehensive 
              qualitative and quantitative evaluations to validate the efficacy of our method. 
            </p>
          </div>
        </div>
      </div>
      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="publication-video">
            <iframe src="https://drive.google.com/file/d/13oALN6RuJGcwhVAVQROJbjDmiQQzwFDe/preview"
                    frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
      <!--/ Paper video. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Pipeline</h2>
      <img src="./static/images/pipeline.png" class="interpolation-image" />
      <p>
        Given a single image input, we introduce a novel pipeline to generate high-fidelity, physics-grounded video with 3D understanding. 
        Our pipeline consists of two main stages: first, we perform a single view 3DGS reconstruction of segmented object from the input image, 
        then synthesize a physics-grounded coarse object dynamics. Next, we apply a difusion-based video enhancement to produce the final enhanced video 
        with backgrounds, enabling users to create visually compelling, physics-driven video from a single image with an applied conditional force or torque.
      </p>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
        <h2 class="title is-3">Comparison</h2>
        <p>
            We compare PhysMotion with several open-sourced state-of-the-art image-to-video generation models: 
            <a href="https://arxiv.org/abs/2311.04145" target="_blank">I2VGen-XL</a>, 
            <a href="https://arxiv.org/abs/2408.06072" target="_blank">CogVideoX-5B</a>, 
            <a href="https://arxiv.org/abs/2401.15977" target="_blank">MotionI2V</a>, 
            <a href="https://arxiv.org/abs/2403.07420" target="_blank">DragAnything</a>, 
            <a href="https://arxiv.org/abs/2310.12190" target="_blank">Dynamicrafter</a>. 
            The first three methods use text-based conditions, while the latter two use trajectory-based conditions to generate dynamics from a single image. 
            We use ChatGPT-4 to generate the prompts for each image on possible dynamics for video generation.
        </p>
        <br>
        <!-- Images section -->
        <figure class="image is-fullwidth">
            <img src=".\static\images\physmotioncomparison-1.png" alt="Comparison Image 1">
        </figure>
        <figure class="image is-fullwidth">
            <img src=".\static\images\physmotionsupcom-1.png" alt="Comparison Image 2">
        </figure>
    </div>
</section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Generative Video Enhancement</h2>
      <p>
        Due to challenges in filling internal Gaussians without internal information as done in <a href="https://arxiv.org/abs/2311.12198" target="_blank">PhyGaussian</a>, 
        the torn area on the left was blurry. Our generative video enhancement pipeline successfully captures the realistic texture of bread in the torn area as shown on the right.
      </p>
      <br>
      <!-- Video comparison -->
      <div style="display: flex; justify-content: space-between; gap: 20px;">
          <!-- Left video -->
          <div style="flex: 1; text-align: center;">
              <video controls style="width: 100%; border: 1px solid #ccc;">
                  <source src=".\static\videos\physgs_bread.mp4" type="video/mp4">
                  Your browser does not support the video tag.
              </video>
              <p>w/o generative video enhancement</p>
          </div>
          <!-- Right video -->
          <div style="flex: 1; text-align: center;">
              <video controls style="width: 100%; border: 1px solid #ccc;">
                  <source src=".\static\videos\ours_bread.mp4" type="video/mp4">
                  Your browser does not support the video tag.
              </video>
              <p>w/ generative video enhancement</p>
          </div>
      </div>
      <!--/ Matting. -->


      <!-- Concurrent Work. -->
      <!-- <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Related Links</h2>

          <div class="content has-text-justified">
            <p>
              There's a lot of excellent work that was introduced around the same time as ours.
            </p>
            <p>
              <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an
              idea similar to our windowed position encoding for coarse-to-fine optimization.
            </p>
            <p>
              <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a
                href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
              both use deformation fields to model non-rigid scenes.
            </p>
            <p>
              Some works model videos with a NeRF by directly modulating the density, such as <a
                href="https://video-nerf.github.io/">Video-NeRF</a>, <a
                href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a
                href="https://neural-3d-video.github.io/">DyNeRF</a>
            </p>
            <p>
              There are probably many more by the time you are reading this. Check out <a
                href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a
                href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
            </p>
          </div>
        </div>
      </div> -->
      <!--/ Concurrent Work. -->

    </div>
  </section>


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{xie2023physgaussian,
      title={PhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics}, 
      author={Xie, Tianyi and Zong, Zeshun and Qiu, Yuxing and Li, Xuan and Feng, Yutao and Yang, Yin and Jiang, Chenfanfu},
      journal={arXiv preprint arXiv:2311.12198},
      year={2023},
}</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="https://arxiv.org/abs/2311.12198">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link external-link" href="#" onclick="return false;" style="cursor: not-allowed; pointer-events: none; color: gray;">
          <i class="fab fa-github"></i>
        </a>        
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              The source code of this website is borrowed from <a
                href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
              We thank the authors for sharing the template.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>